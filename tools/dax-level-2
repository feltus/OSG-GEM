#!/usr/bin/env python

from __future__ import division

import getpass
import sys
import math
import os
import re
import subprocess

run_id = sys.argv[1]
base_dir = sys.argv[2]
run_dir = sys.argv[3]

os.sys.path.insert(0, base_dir + "/tools")

from AutoADAG import *
from Pegasus.DAX3 import *


# globals
task_files = {}



def add_task_files(dax, job, task_name):
    """
    add a set of input files from the task-files dir to a task
    """
    global task_files
    if task_name not in task_files:
        task_files[task_name] = {}
    # add to DAX-level replica catalog
    for fname in os.listdir(base_dir + "/task-files/" + task_name):
        if fname not in task_files[task_name]:
            task_files[task_name][fname] = File(fname)
            task_files[task_name][fname].addPFN( \
                PFN("file://" + base_dir + "/task-files/" + task_name + "/" + fname, "local"))
            dax.addFile(task_files[task_name][fname])
    for f in task_files[task_name]:
        job.uses(f, link=Link.INPUT)



def tophat(dax, base_name, part, common_part, forward_file, reverse_file):

    # Add job
    j = Job(name="tophat")
    j.addArguments(getpass.getuser(), run_id, base_name, part, common_part)
    add_task_files(dax, j, "tophat")
    j.uses(forward_file, link=Link.INPUT)
    j.uses(reverse_file, link=Link.INPUT)
    # output files
    f1 = File(common_part + "-accepted_hits.bam")
    j.uses(f1, link=Link.OUTPUT, transfer=False)
    f2 = File(common_part + "-align_summary.txt")
    j.uses(f2, link=Link.OUTPUT, transfer=True)
    f3 = File(common_part + "-out.txt")
    j.uses(f3, link=Link.OUTPUT, transfer=True)
    dax.addJob(j)

    return f1


def merge(dax, base_name, in_list):

    if len(in_list) == 0:
        print("Error - input list to merge is of 0 length!")
        sys.exit(1)

    # Add job
    j = Job(name="merge")
    j.addArguments(base_name)
    for f in in_list:
        j.uses(f, link=Link.INPUT)
    f = File(base_name + "-merged.bam")
    j.uses(f, link=Link.OUTPUT, transfer=False)
    dax.addJob(j)

    return f


def cuff(dax, base_name, merged):

    # Add job
    j = Job(name="cuff")
    j.addArguments(base_name)
    add_task_files(dax, j, "cuff")
    j.uses(merged, link=Link.INPUT)
    f = File(base_name + "-merged_counts.fpkm")
    j.uses(f, link=Link.OUTPUT, transfer=True)
    dax.addJob(j)

    return f


def generate_sub_wf(base_name, part):
    global  task_files

    task_files = {}
    
    # Create a abstract dag
    dax = AutoADAG("%s-%s" %(base_name, part))
    
    # Add executables to the DAX-level replica catalog
    for exe_name in os.listdir(base_dir + "/tools/"):
        exe = Executable(name=exe_name, arch="x86_64", installed=False)
        exe.addPFN(PFN("file://" + base_dir + "/tools/" + exe_name, "local"))
        if exe_name == "tophat":
            exe.addProfile(Profile(Namespace.PEGASUS, "clusters.size", 5))
        dax.addExecutable(exe)

    my_data_dir = run_dir + "/data/" + base_name + "/" + part

    accepted_hits = []
    for in_name in os.listdir(my_data_dir):

        # only need the forward file
        if not "forward" in in_name:
            continue

        # use stash urls for the data so we can bypass and grab it directly from
        # the jobs
        base_url = "http://stash.osgconnect.net/~" + getpass.getuser() + "/" + run_id + \
                   "/data/" + base_name + "/" + part

        common_part = in_name
        common_part = re.sub("forward\-", "", common_part)
        common_part = re.sub("\.gz", "", common_part)
        
        for_file = File("forward-" + common_part)
        for_file.addPFN(PFN(base_url + "/forward-" + common_part, "stash"))
        dax.addFile(for_file)
        
        rev_file = File("reverse-" + common_part)
        rev_file.addPFN(PFN(base_url + "/reverse-" + common_part, "stash"))
        dax.addFile(rev_file)
    
        out_file = tophat(dax, base_name, part, common_part, for_file, rev_file)
        accepted_hits.append(out_file)

    # merge
    merged = merge(dax, base_name + "-" + part, accepted_hits)

    dax_file = run_dir + "/workflow/" + base_name + "-" + part + ".dax"
    print("Writing " + dax_file)
    f = open(dax_file, "w")
    dax.writeXML(f)
    f.close()

    return dax_file


def main():
    # Create a abstract dag
    dax = AutoADAG("level-2")

    # Add executables to the DAX-level replica catalog
    for exe_name in os.listdir(base_dir + "/tools/"):
        exe = Executable(name=exe_name, arch="x86_64", installed=False)
        exe.addPFN(PFN("file://" + base_dir + "/tools/" + exe_name, "local"))
        dax.addExecutable(exe)

    # has to be the one under public/
    data_dir = "/stash2/user/" + getpass.getuser() + "/public/" + run_id + "/data"

    # we need a bunch of workflows, and one merge/cuff for each base input
    for base_name in os.listdir(data_dir):

        parents = []
        parts = []
        
        for part in os.listdir(data_dir + "/" + base_name):

            print(base_name, part)

            f = generate_sub_wf(base_name, part)
            subdax_file = File(os.path.basename(f))
            subdax_file.addPFN(PFN("file://" + f, "local"))
            dax.addFile(subdax_file)

            # sub workflow job
            j = DAX("%s-%s.dax" %(base_name, part))
            j.addArguments("-Dpegasus.catalog.site.file=%s/sites.xml" % (base_dir),
                           "--sites", "condorpool",
                           "--staging-site", "stash",
                           "--output-site", "local",
                           "--basename", "%s-%s" %(base_name, part),
                           "--cluster", "horizontal",
                           "--force",
                           "--cleanup", "none")
            j.uses(subdax_file, link=Link.INPUT)
            dax.addDAX(j)

            parents.append(j)
            parts.append(part)
            
        # dummy job to get deps for the merge task
        accepted_hits = []
        j = Job(name="merge-bridge")
        j.addArguments(getpass.getuser(), run_id, run_dir, base_name)
        for part in parts:
            f = File(base_name + "-" + part + "-merged.bam")
            j.uses(f, link=Link.OUTPUT, transfer=False)
            accepted_hits.append(f)
        j.addProfile(Profile("hints", "execution.site", "local"))
        dax.addJob(j)
        for p in parents:
            dax.depends(parent=p, child=j)

        # merge
        merged = merge(dax, base_name, accepted_hits)

        # cuff
        cuff(dax, base_name, merged)
    
    # Write the DAX
    f = open(run_dir + "/workflow/level-2.dax", "w")
    dax.writeXML(f)
    f.close()


main()

